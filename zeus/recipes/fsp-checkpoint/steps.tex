\documentclass[letterpaper]{article}
%\documentclass[letterpaper]{scrartcl}
\usepackage[margin=1.0in]{geometry}
\usepackage{graphicx}
\usepackage{mdwlist}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{textcomp}
\usepackage{array}

\lstset{basicstyle=\footnotesize\ttfamily,
  frame=single,
  backgroundcolor=\color{grey},
  showstringspaces=false,
  commentstyle=\color{blue},
  captionpos=b,
  keywordstyle=\color{blue}
}

\newcommand{\master}{master3}
\definecolor{grey}{rgb}{0.96,0.96,0.96}
%\captionsetup[lstlisting]{position=bottom}


\begin{document}

%%%{\hfill\includegraphics[scale=0.14]{../figures/tron-approved}}

%\begin{titlepage}

\begin{center}
\vspace*{-0.5cm}
{\Large Cluster Building Recipes} \\ \vspace*{0.2cm}
{\large -- CentOS6.5 with FSP POC Repository -- } \\ \vspace*{0.75cm}

{\large \em Cluster Maker Team} \\
\today
\end{center}
%\end{titlepage}

%\maketitle

%\vspace*{0.5cm}
\section{Introduction}
This document endeavors to walk through a simple cluster installation using
components from the internal Forest Peak (FSP) 2014 checkpoint. This process is
meant to be repeatable on the Zeus cluster and requires access to {\em RPM}
repositories that are made available by the FSP Open Build Service (OBS)
instance that is used to build and package components. Note: this process is
not meant to dictate a specific, final FSP implementation or installation
direction; instead, it serves as a learning vehicle and {\em proof-of-concept}
highlighting interaction with an FSP repository and coordination with a config
management system. \\

\noindent {\bf Requirements}: this recipe assumes the availability of a single head node
	  {\em master}, and two {\em compute} nodes. The {\em master} node is
	  provisioned with CentOS6.5 and is subsequently configured to
	  provision the remaining {\em compute} nodes with Warewulf in a
	  stateless configuration. We assume an external \$HOME file system is
	  available on master/computes via NFS and that compute node BMCs are
	  available via IPMI from the chosen master host. \\

\noindent {\bf Inputs}: since this recipe details installing a cluster
starting from bare-metal, there is a need to define IP addresses and gather
hardware MAC addresses in order to support controlled provisioning. These values
are hardware and site specific, and this document uses \texttt{<variable>}
names in the command-line examples that follow to highlight where local site
inputs are required. A summary of the required variables used in this recipe
are as follows:

\vspace*{0.2cm}
\begin{tabular}{@{}>{\textbullet}cll@{}}
& \texttt{<nfs\_ip>}:    & IP address of NFS server providing \$HOME \\
& \texttt{<master\_ip>}: & IP address of the master server for the cluster
  being installed\\
& \texttt{<c1\_ip>}:     & IP address of first compute node \\
& \texttt{<c1\_bmc>}:    & BMC address of first compute node \\
& \texttt{<c2\_ip>}:     & IP address of second compute node \\
& \texttt{<c2\_bmc>}:    & BMC address of second compute node \\
\end{tabular}

\section{Install Base Operating System(BOS)}

In an external setting, installing the BOS on a {\em master} host would
typically involve booting from a DVD iso image on a new server.  However, on
the Zeus cluster, master nodes can be network booted with a minimal OS
install using {\em Warewulf}. The Warewulf configuration provisions the OS into
a ramdisk, configures local package repositories to point to the FSP OBS server and
underlying distro repos, and mounts an NFS \$HOME file system. With this setup,
all that is required to achieve a clean BOS intall on a master host is a host
reboot. 

%\newpage
\section{Install FSP Components}

\subsection{Bootstrap}

To begin, bootstrap the master server with the bare essentials necessary to
enable a local configuration management system. The config management system
will then be used in subsequent commands to register additional FSP components
for installation.

\vspace*{0.2cm}
% begin_fsp_run
% fsp_validation_comment Bootstrap

%\begin{lstlisting}[language=bash,caption={Commands run on {\bf master node}.}]
\begin{lstlisting}[language=bash]
[master]$ yum install -y losf
\end{lstlisting}

% end_fsp_run

\subsection{Add baseline FSP and components on {\em master} node}

As an example of the convenience that can be added via combination of config
management and a pre-provided FSP template, the next step adds all the minimum
remaining baseline FSP components necessary for a {\em master} server. We begin
by initializing the config mgmt system with an FSP-tuned template that defines
Warewulf as the underlying config management system and defined two node type
classifications:
\begin{itemize*}
\item master
\item compute (e.g. c1, c2, c3, etc.)
\end{itemize*}
In this demo, we choose to store the cluster config files in /tmp. However, in
normal practice, this would likely be stored in a shared file system (with
config/template files stored in an SCM for versioning).

\vspace*{0.2cm}

% begin_fsp_run
% fsp_validation_comment Add baseline FSP and components on master node

\begin{lstlisting}[language=bash,keywords={}]
[master]$ . /etc/profile.d/losf.sh         # setup env
[master]$ export LOSF_CONFIG_DIR=/tmp/demo # path for configuration setup
[master]$ initconfig cluster FSP_base      # init config with FSP template
[master]$ losf addpkg -y lmod              # adding modules support
[master]$ losf addgroup -y FSP-warewulf    # adding Warewulf support
[master]$ update -q                        # install required components
\end{lstlisting}

% end_fsp_run

%%%Note that on a freshly installed system, the \texttt{update} process above will
%%%download and install all of the pre-defined FSP provided packages (and
%%%associated dependencies) registered in the config mgmt. template.
%%%
\subsection{Complete basic Warewulf setup for {\em master} node}

At this point, all of the packages necessary to use warewulf on the {\em master}
host should be installed.  Next, we need to build the vNFS image, modify a few
settings to work on the Zeus cluster, and provision a host. Example steps are
as follows:

\vspace*{0.2cm}

% begin_fsp_run
% fsp_validation_comment Complete basic Warewulf setup for master node

\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]

[master]$ wwbootstrap `uname -r`          # create warewulf bootstrap image
[master]$ cluster-env                     # create warewulf ssh key

[master]$ export CHROOT=/opt/fsp/admin/images/centos6.5/base

# add new cluster key to base image

[master]$ cat ~/.ssh/cluster.pub >> $CHROOT/root/.ssh/authorized_keys

# add mount of $HOME and /opt/fsp/pub to base image

[master]$ echo "<nfs_ip>:/home /home nfs nfsvers=3 0 0" >> $CHROOT/etc/fstab
[master]$ echo "<master_ip>:/opt/fsp/pub /opt/fsp/pub nfs nfsvers=3 0 0" >> \
    $CHROOT/etc/fstab

# Export /opt/fsp/pub to cluster

[master]$ echo "/opt/fsp/pub <c1_ip>(ro)" >> /etc/exports
[master]$ echo "/opt/fsp/pub <c2_ip>(ro)" >> /etc/exports
[master]$ exportfs -a

# build local vnfs (note: base chroot image provided by FSP)

[master]$ wwvnfs  -y --chroot /opt/fsp/admin/images/centos6.5/base

# Configure warewulf to allow provisioning over eth0

[master]$ perl -pi -e "s/eth1/eth0/"  /etc/warewulf/provision.conf 
\end{lstlisting}

% end_fsp_run

%%%To enable SLURM, the local site administrators would need to create the
%%%\texttt{/etc/slurm/slurm.conf} file, customized to include desired compute
%%%nodes, queue settings, etc. A minimal example to setup SLURM from an example
%%%config file is below:
%%%
%%%\begin{lstlisting}[language=bash,keywords={}]
%%%
%%%# Create minimal SLURM config
%%%
%%%[master]$ head -n -2  /etc/slurm/slurm.conf.example  > /etc/slurm/slurm.conf
%%%[master]$ echo "PropagateResourceLimitsExcept=MEMLOCK" >> /etc/slurm/slurm.conf
%%%[master]$ echo "SlurmdLogFile=/var/log/slurm.log" >> /etc/slurm/slurm.conf
%%%[master]$ echo "NodeName=c[1-2] Sockets=2 CoresPerSocket=8 ThreadsPerCore=2 \
%%%     State=UNKNOWN" >> /etc/slurm/slurm.conf
%%%[master]$ echo "PartitionName=normal Nodes=c[1-2] Default=YES MaxTime=24:00:00 \
%%%     State=UP" >> /etc/slurm/slurm.conf
%%%
%%%[master]$ perl -pi -e "s/ControlMachine=\S+/ControlMachine=master/" 
%%%     /etc/slurm/slurm.conf
%%%\end{lstlisting}

% begin_fsp_run
% fsp_validation_comment Add hosts to cluster

In preparation for provisioning compute nodes, we can now register the desired
network settings for two example compute nodes. Recall that this example is
using the compute nodes assigned to master. Note the use of variables for the
desired comute node IP and MAC addresses which should be modified for local
settings. 

\vspace*{0.2cm}

\begin{lstlisting}[language=bash,keywords={},upquote=true,basicstyle=\footnotesize\ttfamily]

# Define desired compute host credentials within config management

[master]$ echo "c1 <c1_ip> <c1_mac> eth0 255.255.255.0" >> $LOSF_CONFIG_DIR/ips.cluster
[master]$ echo "c2 <c2_ip> <c2_mac> eth0 255.255.255.0" >> $LOSF_CONFIG_DIR/ips.cluster

# Register new compute hosts with provisioning system

[master]$ losf add c1
[master]$ losf add c2

# Define provisioning image for hosts

[master]$ wwsh -y provision set c[1-2] --vnfs=base --bootstrap=`uname -r`

\end{lstlisting}

% end_fsp_run


\subsection{Boot compute nodes}

At this point, the {\em master} server should be able to boot the newly defined
compute nodes.  The service processors for the compute hosts are available via
a separate network on the Zeus cluster. You can point a web browser to their
respective IPs to reboot, or you can issue ipmi commands directly from the {\em
  master} cluster node.  An example to reboot hosts {\em c1} and {\em c2} using
IPMI is shown below.  Note that the \texttt{ireset} command requires that the
\texttt{IPMI\_PASSWORD} variable be set with the local BMC password in order to
work interactively.

\vspace*{0.2cm}

% begin_fsp_run
% fsp_validation_comment Boot compute nodes

\begin{lstlisting}[language=bash,keywords={},upquote=true]
[master]$ ireset <c1_bmc>         # power cycles c1 through BMC IP
[master]$ ireset <c2_bmc>         # power cycles c2 through BMC IP
\end{lstlisting} 

% end_fsp_run

\vspace*{0.25cm}
Once kicked off, the boot process should take less than 2 minutes and you can
verify that the compute hosts are available via ssh, or via parallel ssh tools to multiple
hosts. For example, to wait 100 seconds, and run a command on the newly images
compute hosts:

% begin_fsp_run

\vspace*{0.2cm}
\begin{lstlisting}[language=bash]
[master]$ sleep 100
[master]$ koomie_cf -n compute uptime
c1  14:59:46 up 1 min,  0 users,  load average: 0.12, 0.05, 0.02
c2  14:59:32 up 1 min,  0 users,  load average: 0.23, 0.08, 0.03
\end{lstlisting}

% end_fsp_run

%%%%%%\section{Local Site Config}
%%%
%%%%%%The above sections highlighted the basic steps required to be able to provision
%%%%%%an FSP provided image on compute nodes. The next steps for local site
%%%%%%customization would include:
%%%%%%
%%%%%%\begin{itemize*}
%%%%%%\item Update /etc/fstab for vNFS image to mount \$HOME
%%%%%%\item Create desired SLURM settings in /etc/slurm.conf and distribute
%%%%%%\end{itemize*}
%%%%%%
%%%%\section{Add compilers/MPI}
%%%
%%%%\section{Configure SLURM}
%%%
%%%%\section{Configure and Boot Compute Nodes}
%%%
%%%\section{Install Licenses for Intel Software}
%%%
%%%For internal testing convenience, an RPM containing valid licenses for Intel
%%%compilers and MPI is available. To install, issue:
%%%
%%%\vspace*{0.25cm}
%%%\begin{lstlisting}[language=bash,keywords={},upquote=true]
%%%[master]$ losf addpkg FSP-licenses
%%%[master]$ update -q
%%%\end{lstlisting}
%%%
%%%\section{Run a Test Job}
%%%
%%%At this point, the cluster should be available to run jobs. 
%%%
%%%\begin{lstlisting}[language=bash]
%%%
%%%# Start SLURM on master server
%%%
%%%[master]$ service slurm start
%%%
%%%# Open hosts for production
%%%
%%%[master]$ scontrol  update state=idle nodename=c[1-2]
%%%
%%%# Run a test job as a user
%%%
%%%[master]$ su - kwschulz
%%%[kwschulz@master ~]$ mpicc hello.c
%%%
%%%[kwschulz@master ~]$ srun -n 8 -N 2 ./a.out 
%%%
%%% Hello, world (8 procs total)
%%%    --> Process #   0 of   8 is alive. ->c1
%%%    --> Process #   1 of   8 is alive. ->c1
%%%    --> Process #   2 of   8 is alive. ->c1
%%%    --> Process #   3 of   8 is alive. ->c1
%%%    --> Process #   4 of   8 is alive. ->c2
%%%    --> Process #   5 of   8 is alive. ->c2
%%%    --> Process #   6 of   8 is alive. ->c2
%%%    --> Process #   7 of   8 is alive. ->c2
%%%\end{lstlisting}
%%%
%%% hello world
\end{document}

