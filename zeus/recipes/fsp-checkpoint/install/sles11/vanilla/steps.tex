\documentclass[letterpaper]{article}
\usepackage{../../common/fspdoc}

% Include git variables
\input{vc.tex}

% Define Base OS
\newcommand{\baseOS}{SLES11SP3}
\newcommand{\LosF}{\emph{LosF}}

\begin{document}
\graphicspath{{../../common/figures/}}
\thispagestyle{empty}

%%%{\hfill\includegraphics[scale=0.14]{../figures/tron-approved}}

% Title Page

{\hspace*{4in} \includegraphics[width=1.8in]{intel_1spot_100.pdf}}

\vspace*{2cm}
\noindent {\LARGE \color{RoyalBlue} \fontfamily{phv}\selectfont 2014 Forest Peak Checkpoint} \vspace*{0.1cm} \\
\noindent {\LARGE \color{RoyalBlue} \fontfamily{phv}\selectfont Cluster Building Recipes} \\ 
\noindent\rule{6in}{0.4pt} \\

\noindent {\Large \baseOS{} Base OS} \\ \vspace{0.2cm}

\noindent {\large {\em Base Linux Edition }}

\vspace*{3in}

\noindent{\normalsize Intel Cluster Maker Team} \vspace*{0.1cm} \\
{\normalsize Copyright~{\small\copyright}~2014 Intel Corporation} \vspace*{0.1cm} \\ 
{\normalsize Document Last Update: \VCDateISO} \vspace*{0.1cm} \\ 
{\normalsize Document Revision: \VCRevision} \\ \vspace*{0.1cm}

\newpage
\tableofcontents
\newpage

% Introduction  --------------------------------------------------

\section{Introduction}
\input{../../common/intro} \\

\noindent {\bf Base Linux Addition}: this edition of the guide highlights
installation without the use of a companion configuration management system and
directly uses distro-provided package management tools for component
selection. The steps that follow also highlight specific changes to system
configuration files that are required as part of the cluster install
process. Other editions of this guide provide similar install steps when using
specific configuration management systems that can simplify the installation
and configuration process. \\

\input{../../common/requirements}
\input{../../common/inputs}

% ------------------------------------------------------------------

\section{Install Base Operating System(BOS)}
\input{../../common/bos}

% ------------------------------------------------------------------

\section{Install FSP Components}

With the BOS installed and booted, the next step is to add desired FSP packages
onto the {\em master} server in order to provide provisioning and resource
management services for the rest of the cluster. The following subsections
highlight this process.

\subsection{Enable FSP repository for local use}
\input{../../common/enable_fsp_repo}

\vspace*{0.2cm}
% begin_fsp_run
% fsp_validation_comment Enable FSP Repo
\begin{lstlisting}[language=bash]
[master]$ zypper ar http://10.23.186.191:82/fsp/SLE_11_SP3_Intel/fsp.repo 
\end{lstlisting}

% end_fsp_run

\subsection{Add provisioning services on {\em master} node}

With the FSP repo enabled, we can now begin adding desired components onto the
{\em master} server. The FSP repo provides a number of aliases that group
logical components togetther in order to help aid in this process. To add
support for provisioning services, the following commands illustrate addition
of a common base package followed by the Warewulf provisioning system.

\vspace*{0.2cm}

% begin_fsp_run
% fsp_validation_comment Add baseline FSP and warewulf on master node

\begin{lstlisting}[language=bash,keywords={}]
[master]$ zypper -n install -t pattern FSP-base          # adding base FSP packages
[master]$ zypper -n install -t pattern FSP-warewulf      # adding Warewulf support
\end{lstlisting}

% end_fsp_run



%%%Note that on a freshly installed system, the \texttt{update} process above will
%%%download and install all of the pre-defined FSP provided packages (and
%%%associated dependencies) registered in the config mgmt. template.
%%%
\subsection{Complete basic Warewulf setup for {\em master} node}

At this point, all of the packages necessary to use Warewulf on the {\em
  master} host should be installed.  Next, we need to update several
configuration files in order to allow Warewulf to work with SLES to support
local provisioning using the {\em eth1} interface.  Specific steps are as
follows:

\vspace*{0.2cm}

% begin_fsp_run
% fsp_validation_comment Complete basic Warewulf setup for master node

\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]

# Configure DHCP server to use eth1
[master]$ perl -pi -e 's/^DHCPD_INTERFACE=""/DHCPD_INTERFACE="eth1"/' /etc/sysconfig/dhcpd

# Configure Warewulf to use the default SLES tftp location
[master]$ perl -pi -e "s#\#tftpdir = /var/lib/#tftpdir = /#" /etc/warewulf/provision.conf

# Update Warewulf http config to use the SLES version of mod_perl
[master]$ export MODFILE=/etc/apache2/conf.d/warewulf-httpd.conf
[master]$ perl -pi -e "s#modules/mod_perl.so\$#/usr/lib64/apache2/mod_perl.so#" $MODFILE

# Enable http access for Warewulf cgi-bin directory (add "Allow from all")
[master]$ perl -pi -e "s/cgi-bin>\$/cgi-bin>\n    Allow from all/" $MODFILE

# Define eth1 interface for provisioning
[master]$ ifconfig eth1 <master_ip> netmask 255.255.255.0 up

# Restart relevant services to support provisioning
[master]$ service xinetd restart                         # tftp services
[master]$ service mysql restart                          # mysql database
[master]$ service apache2 restart                        # web server
\end{lstlisting}
% end_fsp_run

\subsection{Define {\em compute} image for provisioning}

With the provisioning services enabled, the next step is to define and
customize a system image that can subsequently be used to provision one or more
{\em compute} nodes. The following subsections highlight this process.

\subsubsection{Build initial BOS image}

The FSP build of Warewulf includes specific enhancements enabling support for
SLES~11. The following steps highlight the process to build a minimal, default
image for use with Warewulf. In this example, the resulting image will be
stored locally on the {\em master} host in the
\texttt{/opt/fsp/admin/images/\baseOS{}} directory. \\

% begin_fsp_run
% fsp_validation_comment Create compute image for Warewulf
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
# Define chroot location for SLES 
[master]$ export CHROOT=/opt/fsp/admin/images/sles11sp3

# Build bootstrap kernel and initial base image
[master]$ wwbootstrap -y `uname -r`           # create warewulf bootstrap image
[master]$ mkdir -p --mode=700 $CHROOT         # create chroot housing dir
[master]$ mkdir -m 755 $CHROOT/dev            # create chroot /dev dir
[master]$ mknod -m 666 $CHROOT/dev/zero c 1 5 # create /dev/zero device
[master]$ wwmkchroot sles-11 $CHROOT          # create base image

\end{lstlisting}
% end_fsp_run

\subsubsection{Add FSP components}

The \texttt{wwmkchroot} process used in the previous step is designed to
provide a minimal SLES configuration. Next, we add addtitional components to
include resource management client services, InfiniBand drivers, and other
additional packages to support the default FSP environment. \\

% begin_fsp_run
% fsp_validation_comment Add FSP components to compute instance
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
# Add SLURM client support
[master]$ chroot $CHROOT zypper -n install -t pattern FSP-slurm-client

# Add IB support
[master]$ chroot $CHROOT zypper -n install ofed
[master]$ chroot $CHROOT zypper -n install libmlx4-rdmav2

# Add Network Time Protocol (NTP) support
[master]$ chroot $CHROOT zypper -n install ntp

# Include FSP default modules user environment
[master]$ chroot $CHROOT zypper -n install lmod 
[master]$ chroot $CHROOT zypper -n install FSP-lmod-defaults-intel
\end{lstlisting}
% end_fsp_run

\subsubsection{Customize system configuration}

Prior to assembling the image, it is advantageous to perform any additional
customizations within the chroot environment created for the desired {\em
  compute} instance. The following steps document the process to add a local
{\em ssh} key created by Warewulf to support remote access, and enable NFS
mounting of the public FSP install path (\texttt{/opt/fsp/pub}) that will be
hosted by the {\em master} host in this example configuration. \\

% begin_fsp_run
% fsp_validation_comment Add ssh key and enable NFS mount of /opt/fsp/pub
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
# add new cluster key to base image

[master]$ cat ~/.ssh/cluster.pub >> $CHROOT/root/.ssh/authorized_keys

# add mount of $HOME and /opt/fsp/pub to base image

[master]$ echo "<master_ip>:/opt/fsp/pub /opt/fsp/pub nfs nfsvers=3 0 0" >> \
    $CHROOT/etc/fstab

# Export /opt/fsp/pub to cluster compute nodes

[master]$ echo "/opt/fsp/pub <c1_ip>(ro)" >> /etc/exports
[master]$ echo "/opt/fsp/pub <c2_ip>(ro)" >> /etc/exports
[master]$ echo "/opt/fsp/pub <c3_ip>(ro)" >> /etc/exports
[master]$ echo "/opt/fsp/pub <c4_ip>(ro)" >> /etc/exports
[master]$ exportfs -a
\end{lstlisting}
% end_fsp_run


\subsubsection{Assemble vNFS image}

% begin_fsp_run
% fsp_validation_comment Assembly vNFS
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
[master]$ wwvnfs -y --chroot $CHROOT
\end{lstlisting}
% end_fsp_run



% Temporarily disabled for master1/2 CI builds

%%% # Configure Warewulf to allow provisioning over eth0
%%% [master]$ perl -pi -e "s/eth1/eth0/"  /etc/warewulf/provision.conf 
%%%[master]$ echo "<nfs_ip>:/home /home nfs nfsvers=3 0 0" >> $CHROOT/etc/fstab

%%%To enable SLURM, the local site administrators would need to create the
%%%\texttt{/etc/slurm/slurm.conf} file, customized to include desired compute
%%%nodes, queue settings, etc. A minimal example to setup SLURM from an example
%%%config file is below:
%%%
%%%\begin{lstlisting}[language=bash,keywords={}]
%%%
%%%# Create minimal SLURM config
%%%
%%%[master]$ head -n -2  /etc/slurm/slurm.conf.example  > /etc/slurm/slurm.conf
%%%[master]$ echo "PropagateResourceLimitsExcept=MEMLOCK" >> /etc/slurm/slurm.conf
%%%[master]$ echo "SlurmdLogFile=/var/log/slurm.log" >> /etc/slurm/slurm.conf
%%%[master]$ echo "NodeName=c[1-2] Sockets=2 CoresPerSocket=8 ThreadsPerCore=2 \
%%%     State=UNKNOWN" >> /etc/slurm/slurm.conf
%%%[master]$ echo "PartitionName=normal Nodes=c[1-2] Default=YES MaxTime=24:00:00 \
%%%     State=UP" >> /etc/slurm/slurm.conf
%%%
%%%[master]$ perl -pi -e "s/ControlMachine=\S+/ControlMachine=master/" 
%%%     /etc/slurm/slurm.conf
%%%\end{lstlisting}

% begin_fsp_run
% fsp_validation_comment Add hosts to cluster

\subsection{Add resource management services on {\em master} node}

The following command adds the SLURM workload manager server components to the
chose {\em master} host. Note that client-side components will be added to a
compute image in a subsequent step.

% begin_fsp_run
% fsp_validation_comment Add resource manager

\begin{lstlisting}[language=bash,keywords={}]
[master]$ zypper -n install -t pattern FSP-slurm-server  # adding SLURM server support
\end{lstlisting}

% end_fsp_run

In preparation for provisioning compute nodes, we can now register the desired
network settings for four example compute nodes. Recall that this example is
using the compute nodes assigned to master. Note the use of variables for the
desired comute node IP and MAC addresses which should be modified for local
settings. 

\vspace*{0.2cm}

\begin{lstlisting}[language=bash,keywords={},upquote=true,basicstyle=\footnotesize\ttfamily]

# Define desired compute host credentials within config management

[master]$ echo "c1 <c1_ip> <c1_mac> eth0 255.255.255.0" >> $LOSF_CONFIG_DIR/ips.cluster
[master]$ echo "c2 <c2_ip> <c2_mac> eth0 255.255.255.0" >> $LOSF_CONFIG_DIR/ips.cluster
[master]$ echo "c3 <c3_ip> <c3_mac> eth0 255.255.255.0" >> $LOSF_CONFIG_DIR/ips.cluster
[master]$ echo "c4 <c4_ip> <c4_mac> eth0 255.255.255.0" >> $LOSF_CONFIG_DIR/ips.cluster

# Register new compute hosts with provisioning system

[master]$ losf add c1
[master]$ losf add c2
[master]$ losf add c3
[master]$ losf add c4

# Restart dhcp 

[master]$ service dhcpd restart

\end{lstlisting}
% end_fsp_run

%%%# Define provisioning image for hosts
%%%[master]$ wwsh -y provision set c[1-2] --vnfs=base --bootstrap=`uname -r`


\subsection{Boot compute nodes}

At this point, the {\em master} server should be able to boot the newly defined
compute nodes.  The service processors for the compute hosts are available via
a separate network on the Zeus cluster. You can point a web browser to their
respective IPs to reboot, or you can issue ipmi commands directly from the {\em
  master} cluster node.  An example to reboot hosts {\em c1} and {\em c2} using
IPMI is shown below.  Note that the \texttt{ireset} command requires that the
\texttt{IPMI\_PASSWORD} variable be set with the local BMC password in order to
work interactively.

\vspace*{0.2cm}

% begin_fsp_run
% fsp_validation_comment Boot compute nodes

\begin{lstlisting}[language=bash,keywords={},upquote=true]
[master]$ ireset <c1_bmc>         # power cycles c1 through BMC IP
[master]$ ireset <c2_bmc>         # power cycles c2 through BMC IP
[master]$ ireset <c3_bmc>         # power cycles c3 through BMC IP
[master]$ ireset <c4_bmc>         # power cycles c4 through BMC IP
\end{lstlisting} 

% end_fsp_run

\vspace*{0.25cm}
Once kicked off, the boot process should take less than 2 minutes and you can
verify that the compute hosts are available via ssh, or via parallel ssh tools to multiple
hosts. For example, to wait 100 seconds, and run a command on the newly imaged
compute hosts:

% begin_fsp_run

\vspace*{0.2cm}
\begin{lstlisting}[language=bash]
[master]$ sleep 100
[master]$ koomie_cf -n compute uptime
c1  05:03am  up   0:02,  0 users,  load average: 0.20, 0.13, 0.05
c2  05:03am  up   0:02,  0 users,  load average: 0.20, 0.14, 0.06
c3  05:08am  up   0:02,  0 users,  load average: 0.19, 0.15, 0.06
c4  05:09am  up   0:02,  0 users,  load average: 0.15, 0.12, 0.05
\end{lstlisting}

% end_fsp_run

%%%%%%\section{Local Site Config}
%%%
%%%%%%The above sections highlighted the basic steps required to be able to provision
%%%%%%an FSP provided image on compute nodes. The next steps for local site
%%%%%%customization would include:
%%%%%%
%%%%%%\begin{itemize*}
%%%%%%\item Update /etc/fstab for vNFS image to mount \$HOME
%%%%%%\item Create desired SLURM settings in /etc/slurm.conf and distribute
%%%%%%\end{itemize*}
%%%%%%
%%%%\section{Add compilers/MPI}
%%%
%%%%\section{Configure SLURM}
%%%
%%%%\section{Configure and Boot Compute Nodes}
%%%
%%%\section{Install Licenses for Intel Software}
%%%
%%%For internal testing convenience, an RPM containing valid licenses for Intel
%%%compilers and MPI is available. To install, issue:
%%%
%%%\vspace*{0.25cm}
%%%\begin{lstlisting}[language=bash,keywords={},upquote=true]
%%%[master]$ losf addpkg FSP-licenses
%%%[master]$ update -q
%%%\end{lstlisting}
%%%
%%%\section{Run a Test Job}
%%%
%%%At this point, the cluster should be available to run jobs. 
%%%
%%%\begin{lstlisting}[language=bash]
%%%
%%%# Start SLURM on master server
%%%
%%%[master]$ service slurm start
%%%
%%%# Open hosts for production
%%%
%%%[master]$ scontrol  update state=idle nodename=c[1-2]
%%%
%%%# Run a test job as a user
%%%
%%%[master]$ su - kwschulz
%%%[kwschulz@master ~]$ mpicc hello.c
%%%
%%%[kwschulz@master ~]$ srun -n 8 -N 2 ./a.out 
%%%
%%% Hello, world (8 procs total)
%%%    --> Process #   0 of   8 is alive. ->c1
%%%    --> Process #   1 of   8 is alive. ->c1
%%%    --> Process #   2 of   8 is alive. ->c1
%%%    --> Process #   3 of   8 is alive. ->c1
%%%    --> Process #   4 of   8 is alive. ->c2
%%%    --> Process #   5 of   8 is alive. ->c2
%%%    --> Process #   6 of   8 is alive. ->c2
%%%    --> Process #   7 of   8 is alive. ->c2
%%%\end{lstlisting}
%%%
%%% hello world

\section{Install FSP Development Components}
\subsection{Compilers}
\subsection{MPI Stacks}
\subsection{3rd Party Libraries}

\section{Run Example Job}

\end{document}

