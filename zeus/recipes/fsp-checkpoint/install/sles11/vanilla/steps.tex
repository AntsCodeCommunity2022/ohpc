\documentclass[letterpaper]{article}
\usepackage{../../common/fspdoc}

% Include git variables
\input{vc.tex}

% Define Base OS
\newcommand{\baseOS}{SLES11SP3}
\newcommand{\LosF}{\emph{LosF}}

\begin{document}
\graphicspath{{../../common/figures/}}
\thispagestyle{empty}

%%%{\hfill\includegraphics[scale=0.14]{../figures/tron-approved}}

% Title Page

{\hspace*{4in} \includegraphics[width=1.8in]{intel_1spot_100.pdf}}

\vspace*{2cm}
\noindent {\LARGE \color{RoyalBlue} \fontfamily{phv}\selectfont 2014 Forest Peak Checkpoint} \vspace*{0.1cm} \\
\noindent {\LARGE \color{RoyalBlue} \fontfamily{phv}\selectfont Cluster Building Recipes} \\ 
\noindent\rule{6in}{0.4pt} \\

\noindent {\Large \baseOS{} Base OS} \\ \vspace{0.2cm}

\noindent {\large {\em Base Linux Edition }}

\vspace*{3in}

\noindent{\normalsize Intel Cluster Maker Team} \vspace*{0.1cm} \\
{\normalsize Copyright~{\small\copyright}~2014 Intel Corporation} \vspace*{0.1cm} \\ 
{\normalsize Document Last Update: \VCDateISO} \vspace*{0.1cm} \\ 
{\normalsize Document Revision: \VCRevision} \\ \vspace*{0.1cm}

\newpage
\tableofcontents
\newpage

% Introduction  --------------------------------------------------

\section{Introduction} \label{sec:introduction}
\input{../../common/intro} \\

\noindent {\bf Base Linux Addition}: this edition of the guide highlights
installation without the use of a companion configuration management system and
directly uses distro-provided package management tools for component
selection. The steps that follow also highlight specific changes to system
configuration files that are required as part of the cluster install
process. Other editions of this guide provide similar install steps when using
specific configuration management systems that can simplify the installation
and configuration process. \\

\input{../../common/requirements}
\input{../../common/inputs}

% ------------------------------------------------------------------

\section{Install Base Operating System(BOS)}
\input{../../common/bos}

% ------------------------------------------------------------------

\section{Install FSP Components} \label{sec:basic_install}

With the BOS installed and booted, the next step is to add desired FSP packages
onto the {\em master} server in order to provide provisioning and resource
management services for the rest of the cluster. The following subsections
highlight this process.

\subsection{Enable FSP repository for local use}
\input{../../common/enable_fsp_repo}

% begin_fsp_run
% fsp_validation_comment Enable FSP Repo
\begin{lstlisting}[language=bash]
[master]$ zypper ar http://10.23.186.191:82/fsp/SLE_11_SP3_Intel/fsp.repo 
\end{lstlisting}

% end_fsp_run

\subsection{Add provisioning services on {\em master} node}

With the FSP repo enabled, we can now begin adding desired components onto the
{\em master} server. The FSP repo provides a number of aliases that group
logical components togetther in order to help aid in this process. To add
support for provisioning services, the following commands illustrate addition
of a common base package followed by the Warewulf provisioning system. 

% begin_fsp_run
% fsp_validation_comment Add baseline FSP and warewulf on master node
\begin{lstlisting}[language=bash,keywords={}]
[master]$ zypper -n install -t pattern FSP-base          # adding base FSP packages
[master]$ zypper -n install -t pattern FSP-warewulf      # adding Warewulf support
\end{lstlisting}
% end_fsp_run

\subsection{Add resource management services on {\em master} node}

The following command adds the SLURM workload manager server components to the
chosen {\em master} host. Note that client-side components will be added to a
the corresponding compute image in a subsequent step.

% begin_fsp_run
% fsp_validation_comment Add resource manager
\begin{lstlisting}[language=bash,keywords={}]
[master]$ zypper -n install -t pattern FSP-slurm-server  # adding SLURM server support
\end{lstlisting}
% end_fsp_run

SLURM requires the delineation of the system user that runs the underlying
resource management daemons. The default configuration file that is supplied
with the FSP build of SLURM identifies this \texttt{SlurmUser} to be a
dedicated user named \texttt{slurm} and this user must exist. 
%The default location identified to save state information is
%\texttt{/var/slurm} and this directory
The following command can be used to add this user to the {\em
  master} server:

% begin_fsp_run
% fsp_validation_comment Add slurm user
\begin{lstlisting}[language=bash,keywords={}]
[master]$ useradd slurm                                  # add "slurm" user
\end{lstlisting}
% end_fsp_run


\subsection{Complete basic Warewulf setup for {\em master} node}

At this point, all of the packages necessary to use Warewulf on the {\em
  master} host should be installed.  Next, we need to update several
configuration files in order to allow Warewulf to work with SLES and to support
local provisioning using the {\em eth1} interface.  Specific steps are as
follows:

% begin_fsp_run
% fsp_validation_comment Complete basic Warewulf setup for master node

\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
# Configure DHCP server to use eth1
[master]$ perl -pi -e 's/^DHCPD_INTERFACE=""/DHCPD_INTERFACE="eth1"/' /etc/sysconfig/dhcpd

# Configure Warewulf to use the default SLES tftp location
[master]$ perl -pi -e "s#\#tftpdir = /var/lib/#tftpdir = /#" /etc/warewulf/provision.conf

# Update Warewulf http config to use the SLES version of mod_perl
[master]$ export MODFILE=/etc/apache2/conf.d/warewulf-httpd.conf
[master]$ perl -pi -e "s#modules/mod_perl.so\$#/usr/lib64/apache2/mod_perl.so#" $MODFILE

# Enable http access for Warewulf cgi-bin directory (add "Allow from all")
[master]$ perl -pi -e "s/cgi-bin>\$/cgi-bin>\n    Allow from all/" $MODFILE

# Define eth1 interface for provisioning
[master]$ ifconfig eth1 <master_ip> netmask 255.255.255.0 up

# Restart relevant services to support provisioning
[master]$ service xinetd restart                         # tftp services
[master]$ service mysql restart                          # mysql database
[master]$ service apache2 restart                        # web server
\end{lstlisting}
% end_fsp_run

\subsection{Define {\em compute} image for provisioning}

With the provisioning services enabled, the next step is to define and
customize a system image that can subsequently be used to provision one or more
{\em compute} nodes. The following subsections highlight this process.

\subsubsection{Build initial BOS image}

The FSP build of Warewulf includes specific enhancements enabling support for
SLES~11. The following steps highlight the process to build a minimal, default
image for use with Warewulf. In this example, the resulting image will be
stored locally on the {\em master} host in the
\texttt{/opt/fsp/admin/images/\baseOS{}} directory.

% begin_fsp_run
% fsp_validation_comment Create compute image for Warewulf
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
# Define chroot location for SLES 
[master]$ export CHROOT=/opt/fsp/admin/images/sles11sp3

# Build bootstrap kernel and initial base image
[master]$ wwbootstrap `uname -r`              # create warewulf bootstrap image
[master]$ mkdir -p --mode=700 $CHROOT         # create chroot housing dir
[master]$ mkdir -m 755 $CHROOT/dev            # create chroot /dev dir
[master]$ mknod -m 666 $CHROOT/dev/zero c 1 5 # create /dev/zero device
[master]$ wwmkchroot sles-11 $CHROOT          # create base image

\end{lstlisting}
% end_fsp_run

\subsubsection{Add FSP components}

The \texttt{wwmkchroot} process used in the previous step is designed to
provide a minimal SLES configuration. Next, we add additional components to
include resource management client services, InfiniBand drivers, and other
additional packages to support the default FSP environment.

% begin_fsp_run
% fsp_validation_comment Add FSP components to compute instance
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
# Add SLURM client support
[master]$ chroot $CHROOT zypper -n install -t pattern FSP-slurm-client

# Add IB support
[master]$ chroot $CHROOT zypper -n install ofed
[master]$ chroot $CHROOT zypper -n install libmlx4-rdmav2

# Add Network Time Protocol (NTP) support
[master]$ chroot $CHROOT zypper -n install ntp

# Include FSP default modules user environment
[master]$ chroot $CHROOT zypper -n install lmod 
[master]$ chroot $CHROOT zypper -n install FSP-lmod-defaults-intel
\end{lstlisting}
% end_fsp_run

\subsubsection{Customize system configuration}

Prior to assembling the image, it is advantageous to perform any additional
customizations within the chroot environment created for the desired {\em
  compute} instance. The following steps document the process to add a local
{\em ssh} key created by Warewulf to support remote access, identify the
resource manager server, and enable NFS mounting of the public FSP
install path (\texttt{/opt/fsp/pub}) that will be hosted by the {\em master}
host in this example configuration. Note that the example shown here
specifically exports the \texttt{/opt/fsp/pub} path to each compute node
individually. In practice, however, a single entry would likely be used to
export the mount point to a private subnet suitably sized to comprise all
desired endpoints. 

% begin_fsp_run
% fsp_validation_comment Add ssh key and enable NFS mount of /opt/fsp/pub
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
# add new cluster key to base image
[master]$ cat ~/.ssh/cluster.pub >> $CHROOT/root/.ssh/authorized_keys

# add mount of $HOME and /opt/fsp/pub to base image
[master]$ echo "<master_ip>:/opt/fsp/pub /opt/fsp/pub nfs nfsvers=3 0 0" >> \
    $CHROOT/etc/fstab

# Identify resource manager server in SLURM config files
[master]$ perl -pi -e "s/ControlMachine=\S+/ControlMachine=<master_host>/" \
    $CHROOT/etc/slurm/slurm.conf
[master]$ perl -pi -e "s/ControlMachine=\S+/ControlMachine=<master_host>/" \
    /etc/slurm/slurm.conf

# Export /opt/fsp/pub to cluster compute nodes
[master]$ echo "/opt/fsp/pub <c1_ip>(ro)" >> /etc/exports
[master]$ echo "/opt/fsp/pub <c2_ip>(ro)" >> /etc/exports
[master]$ echo "/opt/fsp/pub <c3_ip>(ro)" >> /etc/exports
[master]$ echo "/opt/fsp/pub <c4_ip>(ro)" >> /etc/exports
[master]$ exportfs -a
\end{lstlisting}
% end_fsp_run


\subsubsection{Assemble vNFS image}

With the local site customizations in place, the following step uses the
\texttt{wwvnfs} command to assemble a vNFS image from the chroot environment
defined for the {\em compute} instance. 

% begin_fsp_run
% fsp_validation_comment Assembly vNFS
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]
[master]$ wwvnfs -y --chroot $CHROOT
\end{lstlisting}
% end_fsp_run

\subsubsection{Register nodes for provisioning}

In preparation for provisioning, we can now define the desired network settings
for four example compute nodes with the underlying provisioning system and
restart the \texttt{dhcp} service. Note the use of variable names for the
desired compute node IP and MAC addresses which should be modified to
accomodate local settings and hardware. The final step in this process
associates the vNFS image assembled in previous steps with the newly defined
compute nodes.

% begin_fsp_run
% fsp_validation_comment Add hosts to cluster

\begin{lstlisting}[language=bash,keywords={},upquote=true,basicstyle=\footnotesize\ttfamily]
# Define four compoute nodes and network settings 
[master]$ wwsh -y node new c1 --ipaddr=<c1_ip> --hwaddr=<c1_mac> 
[master]$ wwsh -y node new c2 --ipaddr=<c2_ip> --hwaddr=<c2_mac> 
[master]$ wwsh -y node new c3 --ipaddr=<c3_ip> --hwaddr=<c3_mac> 
[master]$ wwsh -y node new c4 --ipaddr=<c4_ip> --hwaddr=<c4_mac> 

# Restart dhcp 
[master]$ service dhcpd restart

# Define provisioning image for hosts
[master]$ wwsh -y provision set c[1-4] --vnfs=sles11sp3 --bootstrap=`uname -r`
\end{lstlisting}
% end_fsp_run

\subsection{Boot compute nodes}

\input{../../common/reset_computes} 
The following commands use the \texttt{ipmitool} utility to initiate power
resets on each of the four compute hosts. Note that the utility requires that
the \texttt{IPMI\_PASSWORD} environment variable be set with the local BMC password in
order to work interactively.

% begin_fsp_run
% fsp_validation_comment Boot compute nodes

\begin{lstlisting}[language=bash,keywords={},upquote=true]
[master]$ ipmitool -E -I lanplus -H <c1_bmc> -U root chassis power reset   # power cycle c1
[master]$ ipmitool -E -I lanplus -H <c2_bmc> -U root chassis power reset   # power cycle c2
[master]$ ipmitool -E -I lanplus -H <c3_bmc> -U root chassis power reset   # power cycle c3
[master]$ ipmitool -E -I lanplus -H <c4_bmc> -U root chassis power reset   # power cycle c4
\end{lstlisting} 

% end_fsp_run

Once kicked off, the boot process should take less than 2 minutes and you can
verify that the compute hosts are available via ssh, or via parallel ssh tools to multiple
hosts. For example, to wait 100 seconds, and run a command on the newly imaged
compute hosts using \texttt{pdsh}, execute the following:

% begin_fsp_run

\begin{lstlisting}[language=bash]
[master]$ sleep 100
[master]$ /opt/fsp/admin/pdsh/bin/pdsh -w c[1-4] uptime
c1  05:03am  up   0:02,  0 users,  load average: 0.20, 0.13, 0.05
c2  05:03am  up   0:02,  0 users,  load average: 0.20, 0.14, 0.06
c3  05:03am  up   0:02,  0 users,  load average: 0.19, 0.15, 0.06
c4  05:03am  up   0:02,  0 users,  load average: 0.15, 0.12, 0.05
\end{lstlisting}

% end_fsp_run

%%%%%%\section{Local Site Config}
%%%
%%%%%%The above sections highlighted the basic steps required to be able to provision
%%%%%%an FSP provided image on compute nodes. The next steps for local site
%%%%%%customization would include:
%%%%%%
%%%%%%\begin{itemize*}
%%%%%%\item Update /etc/fstab for vNFS image to mount \$HOME
%%%%%%\item Create desired SLURM settings in /etc/slurm.conf and distribute
%%%%%%\end{itemize*}
%%%%%%
%%%%\section{Add compilers/MPI}
%%%
%%%%\section{Configure SLURM}
%%%
%%%%\section{Configure and Boot Compute Nodes}
%%%


%%%
%%%\section{Run a Test Job}
%%%
%%%At this point, the cluster should be available to run jobs. 
%%%
%%%\begin{lstlisting}[language=bash]
%%%
%%%# Start SLURM on master server
%%%
%%%[master]$ service slurm start
%%%
%%%# Open hosts for production
%%%
%%%[master]$ scontrol  update state=idle nodename=c[1-2]
%%%
%%%# Run a test job as a user
%%%
%%%[master]$ su - kwschulz
%%%[kwschulz@master ~]$ mpicc hello.c
%%%
%%%[kwschulz@master ~]$ srun -n 8 -N 2 ./a.out 
%%%
%%% Hello, world (8 procs total)
%%%    --> Process #   0 of   8 is alive. ->c1
%%%    --> Process #   1 of   8 is alive. ->c1
%%%    --> Process #   2 of   8 is alive. ->c1
%%%    --> Process #   3 of   8 is alive. ->c1
%%%    --> Process #   4 of   8 is alive. ->c2
%%%    --> Process #   5 of   8 is alive. ->c2
%%%    --> Process #   6 of   8 is alive. ->c2
%%%    --> Process #   7 of   8 is alive. ->c2
%%%\end{lstlisting}
%%%
%%% hello world

\section{Install FSP Development Components}

Once the completion of the basic install procedure outlined in
Section~\ref{sec:basic_install} is complete, additional packages can be added
to provide a flexible HPC development environment including C/C++/Fortran
compilers, MPI stacks, and a variety of 3rd party libraries.

\subsection{Compilers}
\subsection{MPI Stacks}
\subsection{3rd Party Libraries}

\section{Install Licenses for Intel Software}

For internal testing convenience, an RPM containing valid licenses for Intel
compilers and MPI is available. To install, issue:

\vspace*{0.25cm}
\begin{lstlisting}[language=bash,keywords={},upquote=true]
[master]$ losf addpkg FSP-licenses
[master]$ update -q
\end{lstlisting}

\section{Run Example Job}

\end{document}

