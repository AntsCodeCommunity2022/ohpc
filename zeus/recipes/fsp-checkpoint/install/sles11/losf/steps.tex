\documentclass[letterpaper]{article}
\usepackage{../../common/fspdoc}

% Include git variables
\input{vc.tex}

% Define Base OS
\newcommand{\baseOS}{SLES11SP3}
\newcommand{\LosF}{\emph{LosF}}

\begin{document}
\thispagestyle{empty}

%%%{\hfill\includegraphics[scale=0.14]{../figures/tron-approved}}

% Title

\begin{center}
\vspace*{-0.5cm}
{\Large Cluster Building Recipes} \\ \vspace*{0.2cm}
{\large -- \baseOS{} Base OS with Forest Peak 2014 Checkpoint Repository -- } \\ \vspace{0.2cm}
{\large [~{ \em Config Management Edition using LosF}~]} \\ \vspace*{0.75cm}

{\large Intel Cluster Maker Team} \\
\end{center}


% Introduction 

\section{Introduction}
\input{../../common/intro} \\

This edition of the install guide highlights the coordinated use of a {\em
  configuration management} system during the install and configuration
process. In this case, the open-source package, \LosF{}, is used and is
provided as a pre-built component via the FSP repository. The use of config
management systems are common on high-end systems and allow administrators to
capture the state of the cluster configuration for trace-ability and to aid in
the distribution of desired software and settings to predefined host types
(e.g. compute nodes, IO nodes, etc). Combined with the template configuration
provided with the FSP checkpoint, it allows for a relatively simple install and
configuration process that is descibed in subsequent sections. \\

\input{../../common/requirements}
\input{../../common/inputs}


\section{Install Base Operating System(BOS)}
\input{../../common/bos}


%\newpage
\section{Install FSP Components}

\subsection{Bootstrap}

To begin, bootstrap the master server with the bare essentials necessary to
enable a local configuration management system. The config management system
will then be used in subsequent commands to register additional FSP components
for installation.

\vspace*{0.2cm}
% begin_fsp_run
% fsp_validation_comment Bootstrap

\begin{lstlisting}[language=bash]
[master]$ zypper -n install losf
\end{lstlisting}

% end_fsp_run

\subsection{Add baseline FSP and components on {\em master} node}

As an example of the convenience that can be added via combination of config
management and a pre-provided FSP template, the next step adds all the minimum
remaining baseline FSP components necessary for a {\em master} server. We begin
by initializing the config mgmt system with an FSP-tuned template that defines
Warewulf as the underlying provisioning system and defined two node type
classifications:
\begin{itemize*}
\item master
\item compute (e.g. c1, c2, c3, etc.)
\end{itemize*}
In this demo, we choose to store the cluster config files in /tmp. However, in
normal practice, this would likely be stored in a shared file system (with
config/template files stored in an SCM for versioning).

\vspace*{0.2cm}

% begin_fsp_run
% fsp_validation_comment Add baseline FSP and components on master node

\begin{lstlisting}[language=bash,keywords={}]
[master]$ . /etc/profile.d/losf.sh         # setup env
[master]$ export LOSF_CONFIG_DIR=/tmp/demo # path for configuration setup
[master]$ initconfig cluster FSP_template  # init config with FSP template
[master]$ losf addgroup -y FSP-base        # adding base FSP packages
[master]$ losf addgroup -y FSP-warewulf    # adding Warewulf support
[master]$ update -q                        # install required components
\end{lstlisting}

% end_fsp_run

%%%Note that on a freshly installed system, the \texttt{update} process above will
%%%download and install all of the pre-defined FSP provided packages (and
%%%associated dependencies) registered in the config mgmt. template.
%%%
\subsection{Complete basic Warewulf setup for {\em master} node}

At this point, all of the packages necessary to use Warewulf on the {\em
  master} host should be installed.  Next, we need to configure Warewulf to
work with SLES, build bootstrap and vNFS images, customize a few settings to
work on the Zeus cluster, and provision four hosts. Specific steps are as
follows:

\vspace*{0.2cm}

% begin_fsp_run
% fsp_validation_comment Complete basic Warewulf setup for master node

\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]

# Configure DHCP server to use eth1
[master]$ perl -pi -e 's/^DHCPD_INTERFACE=""/DHCPD_INTERFACE="eth1"/' /etc/sysconfig/dhcpd

# Configure Warewulf to use default SLES tftp location
[master]$ perl -pi -e "s#\#tftpdir = /var/lib/#tftpdir = /#" /etc/warewulf/provision.conf

# Update Warewulf http config to use SLES version of mod_perl
[master]$ export MODFILE=/etc/apache2/conf.d/warewulf-httpd.conf
[master]$ perl -pi -e "s#modules/mod_perl.so\$#/usr/lib64/apache2/mod_perl.so#" $MODFILE

# Enable http access for Warewulf cgi-bin directory (add "Allow from all")
[master]$ perl -pi -e "s/cgi-bin>\$/cgi-bin>\n    Allow from all/" $MODFILE

# Define eth1 interface for provisioning
[master]$ ifconfig eth1 <master_ip> netmask 255.255.255.0 up

# Restart relevant services
[master]$ service xinetd restart              # tftp services
[master]$ service mysql restart               # mysql database
[master]$ service apache2 restart             # web server

# Define chroot location for SLES 
[master]$ export CHROOT=/opt/fsp/admin/images/sles11sp3

# Build bootstrap kernel and initial base image
[master]$ wwbootstrap -y `uname -r`           # create warewulf bootstrap image
[master]$ mkdir -p --mode=700 $CHROOT         # create chroot housing dir
[master]$ mkdir -m 755 $CHROOT/dev            # create chroot /dev dir
[master]$ mknod -m 666 $CHROOT/dev/zero c 1 5 # create /dev/zero device
[master]$ wwmkchroot sles-11 $CHROOT          # create base image

# add new cluster key to base image

[master]$ cat ~/.ssh/cluster.pub >> $CHROOT/root/.ssh/authorized_keys

# add mount of $HOME and /opt/fsp/pub to base image

[master]$ echo "<master_ip>:/opt/fsp/pub /opt/fsp/pub nfs nfsvers=3 0 0" >> \
    $CHROOT/etc/fstab

# Export /opt/fsp/pub to cluster

[master]$ echo "/opt/fsp/pub <c1_ip>(ro)" >> /etc/exports
[master]$ echo "/opt/fsp/pub <c2_ip>(ro)" >> /etc/exports
[master]$ echo "/opt/fsp/pub <c3_ip>(ro)" >> /etc/exports
[master]$ echo "/opt/fsp/pub <c4_ip>(ro)" >> /etc/exports
[master]$ exportfs -a

# build local vnfs 

[master]$ wwvnfs -y --chroot $CHROOT

\end{lstlisting}

% end_fsp_run

% Temporarily disabled for master1/2 CI builds

%%% # Configure Warewulf to allow provisioning over eth0
%%% [master]$ perl -pi -e "s/eth1/eth0/"  /etc/warewulf/provision.conf 
%%%[master]$ echo "<nfs_ip>:/home /home nfs nfsvers=3 0 0" >> $CHROOT/etc/fstab

%%%To enable SLURM, the local site administrators would need to create the
%%%\texttt{/etc/slurm/slurm.conf} file, customized to include desired compute
%%%nodes, queue settings, etc. A minimal example to setup SLURM from an example
%%%config file is below:
%%%
%%%\begin{lstlisting}[language=bash,keywords={}]
%%%
%%%# Create minimal SLURM config
%%%
%%%[master]$ head -n -2  /etc/slurm/slurm.conf.example  > /etc/slurm/slurm.conf
%%%[master]$ echo "PropagateResourceLimitsExcept=MEMLOCK" >> /etc/slurm/slurm.conf
%%%[master]$ echo "SlurmdLogFile=/var/log/slurm.log" >> /etc/slurm/slurm.conf
%%%[master]$ echo "NodeName=c[1-2] Sockets=2 CoresPerSocket=8 ThreadsPerCore=2 \
%%%     State=UNKNOWN" >> /etc/slurm/slurm.conf
%%%[master]$ echo "PartitionName=normal Nodes=c[1-2] Default=YES MaxTime=24:00:00 \
%%%     State=UP" >> /etc/slurm/slurm.conf
%%%
%%%[master]$ perl -pi -e "s/ControlMachine=\S+/ControlMachine=master/" 
%%%     /etc/slurm/slurm.conf
%%%\end{lstlisting}

% begin_fsp_run
% fsp_validation_comment Add hosts to cluster

In preparation for provisioning compute nodes, we can now register the desired
network settings for four example compute nodes. Recall that this example is
using the compute nodes assigned to master. Note the use of variables for the
desired comute node IP and MAC addresses which should be modified for local
settings. 

\vspace*{0.2cm}

\begin{lstlisting}[language=bash,keywords={},upquote=true,basicstyle=\footnotesize\ttfamily]

# Define desired compute host credentials within config management

[master]$ echo "c1 <c1_ip> <c1_mac> eth0 255.255.255.0" >> $LOSF_CONFIG_DIR/ips.cluster
[master]$ echo "c2 <c2_ip> <c2_mac> eth0 255.255.255.0" >> $LOSF_CONFIG_DIR/ips.cluster
[master]$ echo "c3 <c3_ip> <c3_mac> eth0 255.255.255.0" >> $LOSF_CONFIG_DIR/ips.cluster
[master]$ echo "c4 <c4_ip> <c4_mac> eth0 255.255.255.0" >> $LOSF_CONFIG_DIR/ips.cluster

# Register new compute hosts with provisioning system

[master]$ losf add c1
[master]$ losf add c2
[master]$ losf add c3
[master]$ losf add c4

# Restart dhcp

[master]$ wwsh dhcp restart

\end{lstlisting}
% end_fsp_run

%%%# Define provisioning image for hosts
%%%[master]$ wwsh -y provision set c[1-2] --vnfs=base --bootstrap=`uname -r`


\subsection{Boot compute nodes}

At this point, the {\em master} server should be able to boot the newly defined
compute nodes.  The service processors for the compute hosts are available via
a separate network on the Zeus cluster. You can point a web browser to their
respective IPs to reboot, or you can issue ipmi commands directly from the {\em
  master} cluster node.  An example to reboot hosts {\em c1} and {\em c2} using
IPMI is shown below.  Note that the \texttt{ireset} command requires that the
\texttt{IPMI\_PASSWORD} variable be set with the local BMC password in order to
work interactively.

\vspace*{0.2cm}

% begin_fsp_run
% fsp_validation_comment Boot compute nodes

\begin{lstlisting}[language=bash,keywords={},upquote=true]
[master]$ ireset <c1_bmc>         # power cycles c1 through BMC IP
[master]$ ireset <c2_bmc>         # power cycles c2 through BMC IP
[master]$ ireset <c3_bmc>         # power cycles c3 through BMC IP
[master]$ ireset <c4_bmc>         # power cycles c4 through BMC IP
\end{lstlisting} 

% end_fsp_run

\vspace*{0.25cm}
Once kicked off, the boot process should take less than 2 minutes and you can
verify that the compute hosts are available via ssh, or via parallel ssh tools to multiple
hosts. For example, to wait 100 seconds, and run a command on the newly images
compute hosts:

% begin_fsp_run

\vspace*{0.2cm}
\begin{lstlisting}[language=bash]
[master]$ sleep 100
[master]$ koomie_cf -n compute uptime
c1  14:59:46 up 1 min,  0 users,  load average: 0.12, 0.05, 0.02
c2  14:59:32 up 1 min,  0 users,  load average: 0.23, 0.08, 0.03
\end{lstlisting}

% end_fsp_run

%%%%%%\section{Local Site Config}
%%%
%%%%%%The above sections highlighted the basic steps required to be able to provision
%%%%%%an FSP provided image on compute nodes. The next steps for local site
%%%%%%customization would include:
%%%%%%
%%%%%%\begin{itemize*}
%%%%%%\item Update /etc/fstab for vNFS image to mount \$HOME
%%%%%%\item Create desired SLURM settings in /etc/slurm.conf and distribute
%%%%%%\end{itemize*}
%%%%%%
%%%%\section{Add compilers/MPI}
%%%
%%%%\section{Configure SLURM}
%%%
%%%%\section{Configure and Boot Compute Nodes}
%%%
%%%\section{Install Licenses for Intel Software}
%%%
%%%For internal testing convenience, an RPM containing valid licenses for Intel
%%%compilers and MPI is available. To install, issue:
%%%
%%%\vspace*{0.25cm}
%%%\begin{lstlisting}[language=bash,keywords={},upquote=true]
%%%[master]$ losf addpkg FSP-licenses
%%%[master]$ update -q
%%%\end{lstlisting}
%%%
%%%\section{Run a Test Job}
%%%
%%%At this point, the cluster should be available to run jobs. 
%%%
%%%\begin{lstlisting}[language=bash]
%%%
%%%# Start SLURM on master server
%%%
%%%[master]$ service slurm start
%%%
%%%# Open hosts for production
%%%
%%%[master]$ scontrol  update state=idle nodename=c[1-2]
%%%
%%%# Run a test job as a user
%%%
%%%[master]$ su - kwschulz
%%%[kwschulz@master ~]$ mpicc hello.c
%%%
%%%[kwschulz@master ~]$ srun -n 8 -N 2 ./a.out 
%%%
%%% Hello, world (8 procs total)
%%%    --> Process #   0 of   8 is alive. ->c1
%%%    --> Process #   1 of   8 is alive. ->c1
%%%    --> Process #   2 of   8 is alive. ->c1
%%%    --> Process #   3 of   8 is alive. ->c1
%%%    --> Process #   4 of   8 is alive. ->c2
%%%    --> Process #   5 of   8 is alive. ->c2
%%%    --> Process #   6 of   8 is alive. ->c2
%%%    --> Process #   7 of   8 is alive. ->c2
%%%\end{lstlisting}
%%%
%%% hello world
\end{document}

