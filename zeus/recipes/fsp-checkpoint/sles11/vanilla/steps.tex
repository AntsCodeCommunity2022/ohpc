\documentclass[letterpaper]{article}
%\documentclass[letterpaper]{scrartcl}
\usepackage[margin=1.0in]{geometry}
\usepackage{graphicx}
\usepackage{mdwlist}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{textcomp}
\usepackage{array}

\lstset{basicstyle=\footnotesize\ttfamily,
  frame=single,
  backgroundcolor=\color{grey},
  showstringspaces=false,
  commentstyle=\color{blue},
  captionpos=b,
  keywordstyle=\color{blue}
}

\newcommand{\master}{master3}
\definecolor{grey}{rgb}{0.96,0.96,0.96}
\newcommand{\baseOS}{SLES11SP3}
%\captionsetup[lstlisting]{position=bottom}


\begin{document}

%%%{\hfill\includegraphics[scale=0.14]{../figures/tron-approved}}

%\begin{titlepage}

\begin{center}
\vspace*{-0.5cm}
{\Large Cluster Building Recipes} \\ \vspace*{0.2cm}
{\large -- \baseOS{} Base OS with FSP 2014 Checkpoint Repository -- } \\ \vspace*{0.75cm}

{\large \em Cluster Maker Team} \\
\today
\end{center}
%\end{titlepage}

%\maketitle

%\vspace*{0.5cm}
\section{Introduction}
This document endeavors to walk through a simple cluster installation using
components from the internal Forest Peak (FSP) 2014 checkpoint. This process is
meant to serve as a repeatable example on the Zeus cluster and requires access
to {RPM} repositories that are made available by the FSP Open Build Service
(OBS) instance that is used to build and package components. Note: this process
is not meant to dictate a specific, final FSP implementation or installation
direction; instead, it serves as a learning vehicle and {\em proof-of-concept}
highlighting interaction with an FSP repository and coordination with a config
management system. \\

\input{../comon/requirements}
\input{../common/inputs}

\section{Install Base Operating System(BOS)}

In an external setting, installing the BOS on a {\em master} host would
typically involve booting from a DVD iso image on a new server.  However, on
the Zeus cluster, master nodes can be network booted with a minimal OS
install using {\em Warewulf}. The Warewulf configuration provisions the OS into
a ramdisk, configures local package repositories to point to the FSP OBS server and
underlying distro repos, and mounts an NFS \$HOME file system. With this setup,
all that is required to achieve a clean BOS intall on a master host is a host
reboot. 

%\newpage
\section{Install FSP Components}

This section highlights the basic steps required to install FSP under SLES
using the \texttt{zypper} package manager to access pre-built FSP pacakges and
by modifying the chosen {\em master} host. 

\vspace*{0.2cm}
% begin_fsp_run
% fsp_validation_comment Add baseline FSP and components on master node
\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={}]
[master]$ zypper install  glibc-locale
[master]$ zypper -n install lmod                     # add modules support
[master]$ zypper -n install -t pattern FSP-warewulf  # add Warewulf support
[master]$ zypper -n install kernel-default-base      # add kernel-base if not installed
\end{lstlisting}

% end_fsp_run

\subsection{Complete basic Warewulf setup for {\em master} node}

At this point, all of the packages necessary to use Warewulf on the {\em
  master} host should be installed.  Next, we need to configure Warewulf to
work with SLES, build bootstrap and vNFS images, customize a few settings to
work on the Zeus cluster, and provision two hosts. Specific steps are as
follows:

\vspace*{0.2cm}

% begin_fsp_run
% fsp_validation_comment Complete basic Warewulf setup for master node

\begin{lstlisting}[language=bash,literate={-}{-}1,keywords={},upquote=true]

# Configure DHCP server to use eth0
[master]$ perl -pi -e 's/^DHCPD_INTERFACE=""/DHCPD_INTERFACE="eth0"/' /etc/sysconfig/dhcpd

# Configure Warewulf to allow provisioning over eth0
[master]$ perl -pi -e "s/eth1/eth0/"  /etc/warewulf/provision.conf 

# Configure Warewulf to use default SLES tftp location
[master]$ perl -pi -e "s#\#tftpdir = /var/lib/#tftpdir = /#" /etc/warewulf/provision.conf

# Restart services
[master]$ service xinetd restart              # enable tftp services
###[master]$ service dhcpd restart               # enable dhcpfg services
[master]$ service mysql restart               # startup mysql database

# Define chroot location for SLES and build bootstrap and base images
[master]$ export CHROOT=/opt/fsp/admin/images/sles11sp3
[master]$ wwbootstrap `uname -r`              # create warewulf bootstrap image
[master]$ mkdir -p --mode=700 $CHROOT         # create chroot housing dir
[master]$ mkdir -m 755 $CHROOT/dev            # create chroot /dev dir
[master]$ mknod -m 666 $CHROOT/dev/zero c 1 5 # create /dev/zero device
[master]$ wwmkchroot sles-11 $CHROOT          # create base image

# Update Warewulf http config to use SLES version of mod_perl
[master]$ export MODFILE=/etc/apache2/conf.d/warewulf-httpd.conf
[master]$ perl -pi -e "s#modules/mod_perl.so\$#/usr/lib64/apache2/mod_perl.so#" $MODFILE

# Enable http access for Warewulf cgi-bin directory (add "Allow from all")
[master]$ perl -pi -e "s/cgi-bin>\$/cgi-bin>\n    Allow from all/" $MODFILE

# create cluster ssh key and add to base image
[master]$ cluster-env             
[master]$ cat ~/.ssh/cluster.pub >> $CHROOT/root/.ssh/authorized_keys

# add mount of $HOME and /opt/fsp/pub to base image

[master]$ echo "<nfs_ip>:/home /home nfs nfsvers=3 0 0" >> $CHROOT/etc/fstab
[master]$ echo "<master_ip>:/opt/fsp/pub /opt/fsp/pub nfs nfsvers=3 0 0" >> $CHROOT/etc/fstab

# Export /opt/fsp/pub to cluster

[master]$ echo "/opt/fsp/pub <c1_ip>(ro)" >> /etc/exports
[master]$ echo "/opt/fsp/pub <c2_ip>(ro)" >> /etc/exports
[master]$ exportfs -a

# build local vnfs (note: base chroot image provided by FSP)

[master]$ wwvnfs  -y --chroot $CHROOT


\end{lstlisting}

% end_fsp_run

% begin_fsp_run
% fsp_validation_comment Add hosts to cluster

In preparation for provisioning compute nodes, we can now register the desired
network settings for two example compute nodes. Recall that this example is
using the compute nodes assigned to master. Note the use of variables for the
desired comute node IP and MAC addresses which should be modified for local
settings. 

\vspace*{0.2cm}

\begin{lstlisting}[language=bash,keywords={},upquote=true,basicstyle=\footnotesize\ttfamily]

# Register two new hosts with Warewulf 

[master]$ wwsh -y node new c1 --netdev=eth0 --ipaddr=<c1_ip> --hwaddr=<c1_mac> -G <nfs_ip>
[master]$ wwsh -y node new c2 --netdev=eth0 --ipaddr=<c2_ip> --hwaddr=<c2_mac> -G <nfs_ip>

# Define provisioning image for hosts

[master]$ wwsh -y provision set c[1-2] --vnfs=sles11sp3 --bootstrap=`uname -r`

\end{lstlisting}

% end_fsp_run


\subsection{Boot compute nodes}

At this point, the {\em master} server should be able to boot the newly defined
compute nodes.  The service processors for the compute hosts are available via
a separate network on the Zeus cluster. You can point a web browser to their
respective IPs to reboot, or you can issue ipmi commands directly from the {\em
  master} cluster node.  An example to reboot hosts {\em c1} and {\em c2} using
IPMI is shown below.  Note that the \texttt{ireset} command requires that the
\texttt{IPMI\_PASSWORD} variable be set with the local BMC password in order to
work interactively.

\vspace*{0.2cm}

% begin_fsp_run
% fsp_validation_comment Boot compute nodes

\begin{lstlisting}[language=bash,keywords={},upquote=true]
[master]$ ireset <c1_bmc>         # power cycles c1 through BMC IP
[master]$ ireset <c2_bmc>         # power cycles c2 through BMC IP
\end{lstlisting} 

% end_fsp_run

\vspace*{0.25cm}
Once kicked off, the boot process should take less than 2 minutes and you can
verify that the compute hosts are available via ssh, or via parallel ssh tools to multiple
hosts. For example, to wait 100 seconds, and run a command on the newly images
compute hosts:

% begin_fsp_run

\vspace*{0.2cm}
\begin{lstlisting}[language=bash]
[master]$ sleep 100
[master]$ koomie_cf -n compute uptime
c1  14:59:46 up 1 min,  0 users,  load average: 0.12, 0.05, 0.02
c2  14:59:32 up 1 min,  0 users,  load average: 0.23, 0.08, 0.03
\end{lstlisting}

% end_fsp_run

%%%%%%\section{Local Site Config}
%%%
%%%%%%The above sections highlighted the basic steps required to be able to provision
%%%%%%an FSP provided image on compute nodes. The next steps for local site
%%%%%%customization would include:
%%%%%%
%%%%%%\begin{itemize*}
%%%%%%\item Update /etc/fstab for vNFS image to mount \$HOME
%%%%%%\item Create desired SLURM settings in /etc/slurm.conf and distribute
%%%%%%\end{itemize*}
%%%%%%
%%%%\section{Add compilers/MPI}
%%%
%%%%\section{Configure SLURM}
%%%
%%%%\section{Configure and Boot Compute Nodes}
%%%
%%%\section{Install Licenses for Intel Software}
%%%
%%%For internal testing convenience, an RPM containing valid licenses for Intel
%%%compilers and MPI is available. To install, issue:
%%%
%%%\vspace*{0.25cm}
%%%\begin{lstlisting}[language=bash,keywords={},upquote=true]
%%%[master]$ losf addpkg FSP-licenses
%%%[master]$ update -q
%%%\end{lstlisting}
%%%
%%%\section{Run a Test Job}
%%%
%%%At this point, the cluster should be available to run jobs. 
%%%
%%%\begin{lstlisting}[language=bash]
%%%
%%%# Start SLURM on master server
%%%
%%%[master]$ service slurm start
%%%
%%%# Open hosts for production
%%%
%%%[master]$ scontrol  update state=idle nodename=c[1-2]
%%%
%%%# Run a test job as a user
%%%
%%%[master]$ su - kwschulz
%%%[kwschulz@master ~]$ mpicc hello.c
%%%
%%%[kwschulz@master ~]$ srun -n 8 -N 2 ./a.out 
%%%
%%% Hello, world (8 procs total)
%%%    --> Process #   0 of   8 is alive. ->c1
%%%    --> Process #   1 of   8 is alive. ->c1
%%%    --> Process #   2 of   8 is alive. ->c1
%%%    --> Process #   3 of   8 is alive. ->c1
%%%    --> Process #   4 of   8 is alive. ->c2
%%%    --> Process #   5 of   8 is alive. ->c2
%%%    --> Process #   6 of   8 is alive. ->c2
%%%    --> Process #   7 of   8 is alive. ->c2
%%%\end{lstlisting}
%%%
%%% hello world
\end{document}

