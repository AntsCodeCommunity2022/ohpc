In section \S\ref{sec:basic_install}, the \SLURM{} resource manager was installed
and configured for use on both the {\em master} host and {\em compute} node
instances. With the cluster nodes up and functional, we can now startup the
resource manager services in preparation for running user jobs. Generally, this
is a two-step process that requires starting up the controller daemons on the {\em
  master} host and the client daemons on each of the {\em compute} hosts.  
%Since the {\em compute} hosts were booted into an image that included the SLURM client
%components, the daemons should already be running on the {\em compute}
%hosts. 
Note that \SLURM{} leverages the use of the {\em munge} library to provide
authentication services and this daemon also needs to be running on all hosts
within the resource management pool. 
%The munge daemons should already
%be running on the {\em compute} subsystem at this point, 
The following commands can be used to startup the necessary services to support
resource management under \SLURM{}.

\vspace*{0.3cm}

% begin_fsp_run
% fsp_comment_header Resource Manager Startup \ref{sec:rms_startup}
\begin{lstlisting}[language=bash]
# start munge and slurm controller on master host
[master]$ service munge start
[master]$ service slurmctld start

#  start munge and slurm clients on compute hosts
[master]$ pdsh -w "${compute_regex} service munge start
[master]$ pdsh -w "${compute_regex} service slurmd start
\end{lstlisting}
% end_fsp_run

In the default configuration, the {\em compute} hosts will be initialized in an
{\em unknown} state. To place the hosts into production such that they are
eligible to schedule user jobs, issue the following:

% begin_fsp_run
\begin{lstlisting}[language=bash]
[master]$ scontrol update nodename="${compute_regex}" state=idle
\end{lstlisting}
% end_fsp_run
