With the resource manager enabled for production usage, users should now be
able to run jobs.  We will add a ``test'' user on the {\em master}
host in \S\ref{sec:add_rm} that can be used to run an example job.

% begin_ohpc_run
\begin{lstlisting}[language=bash,keywords={}]
[master](*\#*) useradd -m test
\end{lstlisting}
% end_ohpc_run

\Warewulf{} installs a utility on the compute nodes to automatically 
synchronize files stored in its databse on the SMS every five minutes. 
We must first update the database with the change we just made to passwd and 
shadow files, and since we are impatient we will trigger the compute node pull.

% begin_ohpc_run
\begin{lstlisting}[language=bash,keywords={}]
# update master warewulf database
[master](*\#*) wwsh file sync passwd shadow group

# manually pull changes on compute nodes
[master](*\#*) pdsh -w c[1-4] /warewulf/bin/wwgetfiles 
\end{lstlisting}
% end_ohpc_run

\OHPC{} includes a simple ``hello-world'' MPI application in the
\texttt{/opt/ohpc/pub/examples} directory that can be used for this quick
compilation and execution. \OHPC{} also provides a companion job-launch
script named \texttt{prun} that is installed in concert with the pre-packaged
MPI toolchains. At present, \OHPC{} is unable to include the PMI process management
server normally included within SLURM which implies that \texttt{srun} cannot be
use for MPI job launch. Instead, native job launch mechanisms provided by the
MPI stacks are utilized and \texttt{prun} abstracts this process for the various
stacks to retain a single launch command. To use the test account to compile and execute the
application interactively through the resource manager, execute the following
(note the use of \texttt{prun} and the indication of the underlying native job
launch mechanism):

\begin{lstlisting}[language=bash,keywords={}]
# switch to "test" user
[master](*\#*) su - test

# Compile MPI "hello world" example
[test@master ~]$ mpicc -o hello -O3 /opt/ohpc/pub/examples/mpi/hello.c

# Submit interactive job request and use prun to launch executable
[test@master ~]$ srun -n 8 -N 2 --pty /bin/bash

[test@c1 ~]$ prun ./hello

[prun] Master compute host = c1
[prun] Launch cmd = mpiexec.hydra -bootstrap slurm ./hello

 Hello, world (8 procs total)
    --> Process #   0 of   8 is alive. -> c1
    --> Process #   4 of   8 is alive. -> c2
    --> Process #   1 of   8 is alive. -> c1
    --> Process #   5 of   8 is alive. -> c2
    --> Process #   2 of   8 is alive. -> c1
    --> Process #   6 of   8 is alive. -> c2
    --> Process #   3 of   8 is alive. -> c1
    --> Process #   7 of   8 is alive. -> c2
\end{lstlisting}

Alternatively, a simple batch script can be written and submitted to the
scheduler using sbatch to run when nodes become available. 

\begin{lstlisting}[language=bash,keywords={}]
[test@master ~]$ cat hello.script
#!/bin/bash
#SBATCH -J hi # Job name
#SBATCH -o job.%j.out # Name of stdout output file (%j expands to jobId)
#SBATCH -N 2 # Total number of nodes requested (16 cores/node)
#SBATCH -n 8 # Total number of mpi tasks requested
#SBATCH -t 01:30:00 # Run time (hh:mm:ss) - 1.5 hours

prun ./hello
[test@master ~]$ sbatch hello.script
Submitted batch job 339
[test@master ~]$ cat job.339.out
[prun] Master compute host = c1
[prun] Launch cmd = mpiexec.hydra -bootstrap slurm hostname
 Hello, world (8 procs total)
    --> Process #   0 of   8 is alive. -> c1
    --> Process #   4 of   8 is alive. -> c2
    --> Process #   1 of   8 is alive. -> c1
    --> Process #   5 of   8 is alive. -> c2
    --> Process #   2 of   8 is alive. -> c1
    --> Process #   6 of   8 is alive. -> c2
    --> Process #   3 of   8 is alive. -> c1
    --> Process #   7 of   8 is alive. -> c2
\end{lstlisting}

